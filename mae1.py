import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import os
import time
import re
from concurrent.futures import ThreadPoolExecutor

st.title("üï∏Ô∏è Scraper Inteligente - Nome + Tipo Personalizado")

# === Configura√ß√µes Personaliz√°veis ===
base_url = st.text_input("üåê URL do site:", value="https://")
palavra_chave = st.text_input("üîë Palavra-chave ou parte do nome do arquivo (Regex):", value="certificado")
extensoes = st.text_input("üìÇ Extens√µes desejadas (pdf, jpg, mp4, etc):", value="pdf,jpg,mp4")
crawl_profundo = st.checkbox("üåê Ativar Crawl Profundo?", value=True)
paths_extra = st.text_area("üìÅ Diret√≥rios adicionais (1 por linha):", "/uploads/\n/files/\n/downloads/")
delay = st.number_input("‚è≤Ô∏è Delay entre requisi√ß√µes:", 0, 5, 1)

DOWNLOADS_DIR = 'arquivos_encontrados'
if not os.path.exists(DOWNLOADS_DIR):
    os.makedirs(DOWNLOADS_DIR)

VISITED = set()

# ==== Fun√ß√µes ====

def testar_caminhos_diretos():
    st.info("üõ†Ô∏è Testando caminhos diretos...")
    for path in paths_extra.strip().splitlines():
        for ext in extensoes.split(','):
            tentativa = urljoin(base_url, f"{path.strip()}{palavra_chave}.{ext.strip()}")
            try:
                resp = requests.get(tentativa, timeout=10)
                if resp.status_code == 200:
                    st.success(f"üéØ Encontrado direto: {tentativa}")
                    salvar_arquivo(tentativa, resp.content)
            except:
                pass

def salvar_arquivo(url, content):
    filename = os.path.basename(urlparse(url).path)
    with open(os.path.join(DOWNLOADS_DIR, filename), 'wb') as f:
        f.write(content)
    with open(os.path.join(DOWNLOADS_DIR, filename), 'rb') as file:
        st.download_button(f"üì• Download {filename}", file, filename=filename)

def analisar_pagina(url, nivel=1):
    if url in VISITED or nivel > 5:
        return
    try:
        VISITED.add(url)
        resp = requests.get(url, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        # Busca arquivos que contenham a palavra-chave + extens√£o
        for tag in soup.find_all('a', href=True):
            href = tag['href']
            full_url = urljoin(url, href)
            if re.search(palavra_chave, href, re.IGNORECASE) and any(href.lower().endswith(ext.strip()) for ext in extensoes.split(',')):
                try:
                    file_resp = requests.get(full_url)
                    if file_resp.status_code == 200:
                        st.success(f"üìÑ Arquivo encontrado: {full_url}")
                        salvar_arquivo(full_url, file_resp.content)
                except:
                    continue
        # Crawl Profundo
        if crawl_profundo:
            for a in soup.find_all('a', href=True):
                next_link = urljoin(url, a['href'])
                if base_url in next_link:
                    time.sleep(delay)
                    analisar_pagina(next_link, nivel+1)
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Erro em {url}: {str(e)}")

# ==== EXECU√á√ÉO ====
if st.button("üöÄ Iniciar Busca e Download"):
    testar_caminhos_diretos()
    analisar_pagina(base_url)
    st.success("‚úÖ Processo finalizado!")
